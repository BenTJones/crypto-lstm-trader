{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e69a13",
   "metadata": {},
   "source": [
    "Testing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b68bd",
   "metadata": {},
   "source": [
    "This notebook will be used to import and run the programmed scripts to keep outputs contained into just this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2bed6",
   "metadata": {},
   "source": [
    "0. Initial import of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203ea872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benjo\\Documents\\Projects\\crypto-lstm-trader\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24e7f94f2d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(os.getcwd())\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250c5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.fetch_binance import fetch_binance\n",
    "from preprocessing.prep_data import feature_creation, z_score_norm, window_creation, train_val_test_split, target_def_knext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275036c",
   "metadata": {},
   "source": [
    "1. Obtaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85d8aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prexisting data from ./data/usd_btc_binance.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>3701.23</td>\n",
       "      <td>3713.00</td>\n",
       "      <td>3689.88</td>\n",
       "      <td>3700.31</td>\n",
       "      <td>686.367420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>3700.20</td>\n",
       "      <td>3702.73</td>\n",
       "      <td>3684.22</td>\n",
       "      <td>3689.69</td>\n",
       "      <td>613.539115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>3689.67</td>\n",
       "      <td>3695.95</td>\n",
       "      <td>3675.04</td>\n",
       "      <td>3690.00</td>\n",
       "      <td>895.302181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 03:00:00</th>\n",
       "      <td>3690.00</td>\n",
       "      <td>3699.77</td>\n",
       "      <td>3685.78</td>\n",
       "      <td>3693.13</td>\n",
       "      <td>796.714818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 04:00:00</th>\n",
       "      <td>3692.32</td>\n",
       "      <td>3720.00</td>\n",
       "      <td>3685.94</td>\n",
       "      <td>3692.71</td>\n",
       "      <td>1317.452909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high      low    close       volume\n",
       "timestamp                                                           \n",
       "2019-01-01 00:00:00  3701.23  3713.00  3689.88  3700.31   686.367420\n",
       "2019-01-01 01:00:00  3700.20  3702.73  3684.22  3689.69   613.539115\n",
       "2019-01-01 02:00:00  3689.67  3695.95  3675.04  3690.00   895.302181\n",
       "2019-01-01 03:00:00  3690.00  3699.77  3685.78  3693.13   796.714818\n",
       "2019-01-01 04:00:00  3692.32  3720.00  3685.94  3692.71  1317.452909"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fetch_binance(\n",
    "    exchange_ticker='BTC/USDT',\n",
    "    start_date='2019-01-01T00:00:00Z',\n",
    "    timeframe='1h',\n",
    "    cache_path=('./data/usd_btc_binance.csv'),\n",
    "    max_age_hrs=3\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e3598",
   "metadata": {},
   "source": [
    "2. Normalisation and Window Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0f85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([57788, 48, 41]), torch.Size([57788]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = target_def_knext(df,4)\n",
    "mod_df = feature_creation(df)\n",
    "\n",
    "norm_df = z_score_norm(mod_df, train_frac= 0.7)\n",
    "\n",
    "x, y = window_creation(norm_df, window_size=48)\n",
    "\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90dd6fa",
   "metadata": {},
   "source": [
    "3. Splitting data into test, validation and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23354e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40451, 8668, 8669]\n",
      "[40451, 8668, 8669]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val,x_test, y_test = train_val_test_split(x, y)\n",
    "x_lengths = map(len,[x_train,x_val,x_test])\n",
    "y_lengths = map(len,[y_train,y_val,y_test])\n",
    "print(list(x_lengths))\n",
    "print(list(y_lengths)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ce345",
   "metadata": {},
   "source": [
    "4. Wrap the data in custom datasets so torch dataloaders can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7784cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets.sequence_dataset import SequenceDataset\n",
    "\n",
    "batch_size= 64\n",
    "train_loader= DataLoader(SequenceDataset(x_train,y_train),batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(SequenceDataset(x_val,y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(SequenceDataset(x_test,y_test),batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45c314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "Samples=40451, Positives=20761.0 (0.513)\n",
      "Val split:\n",
      "Samples=8668, Positives=4483.0 (0.517)\n"
     ]
    }
   ],
   "source": [
    "def inspect_split(loader):\n",
    "    n = 0; pos = 0\n",
    "    for _, y in loader:\n",
    "        y = y\n",
    "        n += y.numel()\n",
    "        pos += y.sum().item()\n",
    "    p = pos / n\n",
    "    print(f\"Samples={n}, Positives={pos} ({p:.3f})\")\n",
    "    return p\n",
    "\n",
    "print(\"Train split:\"); p_train = inspect_split(train_loader)\n",
    "print(\"Val split:\");   p_val   = inspect_split(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49b48d",
   "metadata": {},
   "source": [
    "5. Model set up and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results of batch 1Loss : 0.690724, Accuracy : 0.515625Pre-Clip param norm: 0.064924\n",
      "Training results of batch 100Loss : 0.687187, Accuracy : 0.453125Pre-Clip param norm: 0.070381\n",
      "Training results of batch 200Loss : 0.684631, Accuracy : 0.515625Pre-Clip param norm: 0.050966\n",
      "Training results of batch 300Loss : 0.710463, Accuracy : 0.390625Pre-Clip param norm: 0.152891\n",
      "Training results of batch 400Loss : 0.680309, Accuracy : 0.515625Pre-Clip param norm: 0.053216\n",
      "Training results of batch 500Loss : 0.676601, Accuracy : 0.578125Pre-Clip param norm: 0.118471\n",
      "Training results of batch 600Loss : 0.689472, Accuracy : 0.609375Pre-Clip param norm: 0.137983\n",
      "Epoch num: 1  Train: loss 0.6888, acc 0.5365 | Val: loss 0.6875, acc 0.5430, ROC-AUC 0.556, PR-AUC 0.572 | Best Thr(F1) 0.415, F1 0.682\n",
      "New Model saved: Loss = 0.6875 \n",
      "Training results of batch 1Loss : 0.678188, Accuracy : 0.546875Pre-Clip param norm: 0.150952\n",
      "Training results of batch 100Loss : 0.693464, Accuracy : 0.531250Pre-Clip param norm: 0.099013\n",
      "Training results of batch 200Loss : 0.678898, Accuracy : 0.546875Pre-Clip param norm: 0.079600\n",
      "Training results of batch 300Loss : 0.675615, Accuracy : 0.531250Pre-Clip param norm: 0.196303\n",
      "Training results of batch 400Loss : 0.736562, Accuracy : 0.406250Pre-Clip param norm: 0.251179\n",
      "Training results of batch 500Loss : 0.682901, Accuracy : 0.562500Pre-Clip param norm: 0.102258\n",
      "Training results of batch 600Loss : 0.715499, Accuracy : 0.515625Pre-Clip param norm: 0.227635\n",
      "Epoch num: 2  Train: loss 0.6831, acc 0.5565 | Val: loss 0.6909, acc 0.5321, ROC-AUC 0.555, PR-AUC 0.567 | Best Thr(F1) 0.354, F1 0.683\n",
      "Training results of batch 1Loss : 0.681900, Accuracy : 0.515625Pre-Clip param norm: 0.149359\n",
      "Training results of batch 100Loss : 0.711389, Accuracy : 0.500000Pre-Clip param norm: 0.190133\n",
      "Training results of batch 200Loss : 0.672639, Accuracy : 0.625000Pre-Clip param norm: 0.155659\n",
      "Training results of batch 300Loss : 0.671871, Accuracy : 0.609375Pre-Clip param norm: 0.080903\n",
      "Training results of batch 400Loss : 0.686373, Accuracy : 0.562500Pre-Clip param norm: 0.270240\n",
      "Training results of batch 500Loss : 0.694612, Accuracy : 0.546875Pre-Clip param norm: 0.124367\n",
      "Training results of batch 600Loss : 0.641668, Accuracy : 0.671875Pre-Clip param norm: 0.312776\n",
      "Epoch num: 3  Train: loss 0.6807, acc 0.5621 | Val: loss 0.6889, acc 0.5341, ROC-AUC 0.566, PR-AUC 0.574 | Best Thr(F1) 0.385, F1 0.683\n",
      "Training results of batch 1Loss : 0.653260, Accuracy : 0.609375Pre-Clip param norm: 0.165299\n",
      "Training results of batch 100Loss : 0.698208, Accuracy : 0.484375Pre-Clip param norm: 0.162233\n",
      "Training results of batch 200Loss : 0.639069, Accuracy : 0.640625Pre-Clip param norm: 0.141950\n",
      "Training results of batch 300Loss : 0.682376, Accuracy : 0.562500Pre-Clip param norm: 0.221987\n",
      "Training results of batch 400Loss : 0.647985, Accuracy : 0.656250Pre-Clip param norm: 0.157278\n",
      "Training results of batch 500Loss : 0.700772, Accuracy : 0.500000Pre-Clip param norm: 0.198572\n",
      "Training results of batch 600Loss : 0.688984, Accuracy : 0.531250Pre-Clip param norm: 0.228719\n",
      "Epoch num: 4  Train: loss 0.6787, acc 0.5689 | Val: loss 0.6922, acc 0.5284, ROC-AUC 0.562, PR-AUC 0.570 | Best Thr(F1) 0.338, F1 0.683\n",
      "Training results of batch 1Loss : 0.685817, Accuracy : 0.562500Pre-Clip param norm: 0.168403\n",
      "Training results of batch 100Loss : 0.685284, Accuracy : 0.546875Pre-Clip param norm: 0.131123\n",
      "Training results of batch 200Loss : 0.673948, Accuracy : 0.593750Pre-Clip param norm: 0.278327\n",
      "Training results of batch 300Loss : 0.682067, Accuracy : 0.562500Pre-Clip param norm: 0.166248\n",
      "Training results of batch 400Loss : 0.695540, Accuracy : 0.531250Pre-Clip param norm: 0.323846\n",
      "Training results of batch 500Loss : 0.610386, Accuracy : 0.609375Pre-Clip param norm: 0.511812\n",
      "Training results of batch 600Loss : 0.705245, Accuracy : 0.453125Pre-Clip param norm: 0.250007\n",
      "Epoch num: 5  Train: loss 0.6737, acc 0.5768 | Val: loss 0.6880, acc 0.5451, ROC-AUC 0.564, PR-AUC 0.573 | Best Thr(F1) 0.338, F1 0.682\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMClassifier\n",
    "from train.train_model import fit , positive_weight, select_threshold_constrained, threshold_free_metrics\n",
    "\n",
    "input_size = x_train.size(-1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(input_size=input_size,hidden_size=128,num_layers= 2, dropout= 0.1)\n",
    "pos_weight = positive_weight(y_train,device)\n",
    "model,t_star,history = fit(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=5,\n",
    "        lr=1e-3,              \n",
    "        device=device,\n",
    "        save_path=\"models/best.pt\",\n",
    "        patience= 7,            # optional early stopping\n",
    "        pos_weight= None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796c8c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t* (chosen on validation): 0.338\n",
      "TEST ROC-AUC: 0.5423232680012052\n",
      "TEST PR-AUC: 0.5582740825008833\n",
      "TEST F1: 0.682022044849867\n",
      "TEST Precision: 0.5174760641365787\n",
      "TEST Recall: 1.0\n",
      "TEST Confusion Matrix:\n",
      " [[   0 4183]\n",
      " [   0 4486]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "probs_test, ys_test = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        # probs in [0,1]\n",
    "        probs_test.append(model(xb.to(device)).sigmoid().cpu().numpy())\n",
    "        ys_test.append(yb.numpy())\n",
    "\n",
    "probs_test = np.concatenate(probs_test).ravel()\n",
    "ys_test   = np.concatenate(ys_test).ravel()\n",
    "\n",
    "# Lock the val-chosen threshold for test\n",
    "yhat_test = (probs_test >= t_star).astype(float)\n",
    "\n",
    "print(f\"t* (chosen on validation): {t_star:.3f}\")\n",
    "print(\"TEST ROC-AUC:\", roc_auc_score(ys_test, probs_test))\n",
    "print(\"TEST PR-AUC:\",  average_precision_score(ys_test, probs_test))\n",
    "print(\"TEST F1:\",      f1_score(ys_test, yhat_test))\n",
    "print(\"TEST Precision:\", precision_score(ys_test, yhat_test))\n",
    "print(\"TEST Recall:\",    recall_score(ys_test, yhat_test))\n",
    "print(\"TEST Confusion Matrix:\\n\", confusion_matrix(ys_test, yhat_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2852f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE (all positive)\n",
      "F1: 0.682022044849867\n",
      "Precision: 0.5174760641365787\n",
      "Recall: 1.0\n",
      "ROC-AUC: 0.5\n",
      "PR-AUC (≈ base rate): 0.5174761\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Baseline: predict all positives\n",
    "yhat_allpos = np.ones_like(ys_test)\n",
    "\n",
    "print(\"BASELINE (all positive)\")\n",
    "print(\"F1:\", f1_score(ys_test, yhat_allpos))\n",
    "print(\"Precision:\", precision_score(ys_test, yhat_allpos))\n",
    "print(\"Recall:\", recall_score(ys_test, yhat_allpos))\n",
    "print(\"ROC-AUC:\", 0.5)\n",
    "print(\"PR-AUC (≈ base rate):\", ys_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325a9c4",
   "metadata": {},
   "source": [
    "Beginning of HyperTuning - Testing to see the model can learn and if changing things elsewhere (data/features/splitting/optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0050 | loss 0.0006 | acc 1.000\n",
      "0100 | loss 0.0001 | acc 1.000\n",
      "0150 | loss 0.0001 | acc 1.000\n",
      "0200 | loss 0.0000 | acc 1.000\n",
      "0250 | loss 0.0000 | acc 1.000\n",
      "0300 | loss 0.0000 | acc 1.000\n",
      "0350 | loss 0.0000 | acc 1.000\n",
      "0400 | loss 0.0000 | acc 1.000\n",
      "0450 | loss 0.0000 | acc 1.000\n",
      "0500 | loss 0.0000 | acc 1.000\n"
     ]
    }
   ],
   "source": [
    "def tiny_overfit(model, train_ds, steps=500, k=64, lr=3e-3):\n",
    "    from torch.optim import AdamW\n",
    "    from torch.nn.utils import clip_grad_norm_\n",
    "    import random, torch, torch.nn as nn\n",
    "    idx = torch.randperm(len(train_ds))[:k]\n",
    "    X_small = []; y_small = []\n",
    "    for i in idx:\n",
    "        X, y = train_ds[i]\n",
    "        X_small.append(X.unsqueeze(0)); y_small.append(y)\n",
    "    X_small = torch.cat(X_small, dim=0)  # [k, T, F]\n",
    "    y_small = torch.tensor(y_small).float()  # [k]\n",
    "\n",
    "    model.train()\n",
    "    opt = AdamW(model.parameters(), lr=lr)\n",
    "    crit = torch.nn.BCEWithLogitsLoss()\n",
    "    for t in range(steps):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(X_small.to(device))\n",
    "        loss = crit(logits.squeeze(-1), y_small.to(device))\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        if (t+1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                p = (logits.squeeze(-1).sigmoid()>0.5).float()\n",
    "                acc = (p.cpu()==y_small).float().mean().item()\n",
    "            print(f'{t+1:04d} | loss {loss.item():.4f} | acc {acc:.3f}')\n",
    "\n",
    "tiny_overfit(\n",
    "    model=model,\n",
    "    train_ds=SequenceDataset(x_train,y_train),\n",
    "    steps=500,   # updates, not epochs\n",
    "    k=64,        # number of samples to memorize\n",
    "    lr=3e-3      # slightly higher LR for speed\n",
    ")\n",
    "\n",
    "#FROM THIS CELLS OUTPUT OF AN ACCURACY OF 1+ CONSISTENLY AND LOSS OF 0  SO MODEL CAN DEFINETLY LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b55485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_tabular_last(norm_df, window_size=48):\n",
    "    feats = norm_df.drop(columns=['label'])\n",
    "    y = norm_df['label'].to_numpy().astype(int)\n",
    "    Xrows, yrows = [], []\n",
    "    for i in range(len(norm_df) - window_size + 1):\n",
    "        j = i + window_size - 1\n",
    "        Xrows.append(feats.iloc[j].to_numpy())\n",
    "        yrows.append(y[j])\n",
    "    X = np.vstack(Xrows); y = np.array(yrows)\n",
    "    return X, y\n",
    "\n",
    "def chrono_split(X, y, train_frac=0.7, val_frac=0.15):\n",
    "    n = len(y)\n",
    "    i_tr = int(n * train_frac)\n",
    "    i_v  = int(n * (train_frac + val_frac))\n",
    "    return (X[:i_tr], y[:i_tr]), (X[i_tr:i_v], y[i_tr:i_v]), (X[i_v:], y[i_v:])\n",
    "\n",
    "def eval_with_constraints(probs, y, min_pos_rate=0.05, max_pos_rate=0.95, min_precision=None):\n",
    "    roc, pr = threshold_free_metrics(probs, y)\n",
    "    tinfo = select_threshold_constrained(probs, y, min_pos_rate, max_pos_rate, min_precision)\n",
    "    if tinfo[\"f1\"] < 0:\n",
    "        return {\"roc\": roc, \"pr\": pr, \"t\": None, \"f1\": None, \"prec\": None, \"rec\": None, \"pos_rate\": None}\n",
    "    yhat = (probs >= tinfo[\"t\"])\n",
    "    return {\n",
    "        \"roc\": roc, \"pr\": pr, \"t\": tinfo[\"t\"], \"f1\": tinfo[\"f1\"],\n",
    "        \"prec\": precision_score(y, yhat, zero_division=0),\n",
    "        \"rec\":  recall_score(y, yhat, zero_division=0),\n",
    "        \"pos_rate\": float(yhat.mean())\n",
    "    }\n",
    "\n",
    "def logistic_baseline(norm_df, window_size=48, C=1.0, max_iter=2000):\n",
    "    X, y = make_tabular_last(norm_df, window_size)\n",
    "    (Xtr, ytr), (Xv, yv), (Xte, yte) = chrono_split(X, y, 0.7, 0.15)\n",
    "    cls_wt = \"balanced\" if (ytr.mean() < 0.35 or ytr.mean() > 0.65) else None\n",
    "    lr = LogisticRegression(C=C, max_iter=max_iter, solver=\"lbfgs\", class_weight=cls_wt)\n",
    "    lr.fit(Xtr, ytr)\n",
    "\n",
    "    pv = lr.predict_proba(Xv)[:,1]\n",
    "    min_prec = max(0.55, float(yv.mean()))\n",
    "    val_metrics = eval_with_constraints(pv, yv, 0.05, 0.95, min_prec)\n",
    "\n",
    "    pt = lr.predict_proba(Xte)[:,1]\n",
    "    if val_metrics[\"t\"] is not None:\n",
    "        yhat_t = (pt >= val_metrics[\"t\"])\n",
    "        test_metrics = {\n",
    "            \"roc\": roc_auc_score(yte, pt),\n",
    "            \"pr\":  average_precision_score(yte, pt),\n",
    "            \"f1\":  f1_score(yte, yhat_t, zero_division=0),\n",
    "            \"prec\": precision_score(yte, yhat_t, zero_division=0),\n",
    "            \"rec\":  recall_score(yte, yhat_t, zero_division=0),\n",
    "            \"pos_rate\": float(yhat_t.mean()),\n",
    "            \"t\": val_metrics[\"t\"],\n",
    "        }\n",
    "    else:\n",
    "        test_metrics = {\"roc\": roc_auc_score(yte, pt), \"pr\": average_precision_score(yte, pt),\n",
    "                        \"f1\": None, \"prec\": None, \"rec\": None, \"pos_rate\": None, \"t\": None}\n",
    "    return val_metrics, test_metrics\n",
    "\n",
    "val_m, test_m = logistic_baseline(norm_df, window_size=48)\n",
    "print(\"LogReg VAL:\", val_m)\n",
    "print(\"LogReg TEST:\", test_m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
