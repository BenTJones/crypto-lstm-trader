{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e69a13",
   "metadata": {},
   "source": [
    "Testing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b68bd",
   "metadata": {},
   "source": [
    "This notebook will be used to import and run the programmed scripts to keep outputs contained into just this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2bed6",
   "metadata": {},
   "source": [
    "0. Initial import of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203ea872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benjo\\Documents\\Projects\\crypto-lstm-trader\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2987eb9f2d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(os.getcwd())\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.fetch_binance import fetch_binance\n",
    "from preprocessing.prep_data import feature_creation, z_score_norm, window_creation, train_val_test_split, make_label_k_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275036c",
   "metadata": {},
   "source": [
    "1. Obtaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85d8aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prexisting data from ./data/usd_btc_binance.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-17 04:00:00</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>4313.62</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4308.83</td>\n",
       "      <td>47.181009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 05:00:00</th>\n",
       "      <td>4308.83</td>\n",
       "      <td>4328.69</td>\n",
       "      <td>4291.37</td>\n",
       "      <td>4315.32</td>\n",
       "      <td>23.234916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 06:00:00</th>\n",
       "      <td>4330.29</td>\n",
       "      <td>4345.45</td>\n",
       "      <td>4309.37</td>\n",
       "      <td>4324.35</td>\n",
       "      <td>7.229691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 07:00:00</th>\n",
       "      <td>4316.62</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>4287.41</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>4.443249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 08:00:00</th>\n",
       "      <td>4333.32</td>\n",
       "      <td>4377.85</td>\n",
       "      <td>4333.32</td>\n",
       "      <td>4360.69</td>\n",
       "      <td>0.972807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high      low    close     volume\n",
       "timestamp                                                         \n",
       "2017-08-17 04:00:00  4261.48  4313.62  4261.32  4308.83  47.181009\n",
       "2017-08-17 05:00:00  4308.83  4328.69  4291.37  4315.32  23.234916\n",
       "2017-08-17 06:00:00  4330.29  4345.45  4309.37  4324.35   7.229691\n",
       "2017-08-17 07:00:00  4316.62  4349.99  4287.41  4349.99   4.443249\n",
       "2017-08-17 08:00:00  4333.32  4377.85  4333.32  4360.69   0.972807"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fetch_binance(\n",
    "    exchange_ticker='BTC/USDT',\n",
    "    start_date='2017-01-01T00:00:00Z',\n",
    "    timeframe='1h',\n",
    "    cache_path=('./data/usd_btc_binance.csv'),\n",
    "    max_age_hrs=3\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e3598",
   "metadata": {},
   "source": [
    "2. Normalisation and Window Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24796, 48, 41]), torch.Size([24796]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#df = target_def_knext(df,4)\n",
    "labeled_df = make_label_k_epsilon(df,12,0.7,0.7,True)\n",
    "mod_df = feature_creation(labeled_df)\n",
    "\n",
    "norm_df = z_score_norm(mod_df, train_frac= 0.7)\n",
    "\n",
    "x, y = window_creation(norm_df, window_size=48)\n",
    "\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90dd6fa",
   "metadata": {},
   "source": [
    "3. Splitting data into test, validation and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23354e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17357, 3719, 3720]\n",
      "[17357, 3719, 3720]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val,x_test, y_test = train_val_test_split(x, y)\n",
    "x_lengths = map(len,[x_train,x_val,x_test])\n",
    "y_lengths = map(len,[y_train,y_val,y_test])\n",
    "print(list(x_lengths))\n",
    "print(list(y_lengths)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ce345",
   "metadata": {},
   "source": [
    "4. Wrap the data in custom datasets so torch dataloaders can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7784cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets.sequence_dataset import SequenceDataset\n",
    "\n",
    "batch_size= 64\n",
    "train_loader= DataLoader(SequenceDataset(x_train,y_train),batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(SequenceDataset(x_val,y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(SequenceDataset(x_test,y_test),batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45c314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "Samples=17357, Positives=9179.0 (0.529)\n",
      "Val split:\n",
      "Samples=3719, Positives=1991.0 (0.535)\n"
     ]
    }
   ],
   "source": [
    "def inspect_split(loader):\n",
    "    n = 0; pos = 0\n",
    "    for _, y in loader:\n",
    "        y = y\n",
    "        n += y.numel()\n",
    "        pos += y.sum().item()\n",
    "    p = pos / n\n",
    "    print(f\"Samples={n}, Positives={pos} ({p:.3f})\")\n",
    "    return p\n",
    "\n",
    "print(\"Train split:\"); p_train = inspect_split(train_loader)\n",
    "print(\"Val split:\");   p_val   = inspect_split(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49b48d",
   "metadata": {},
   "source": [
    "5. Model set up and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results of batch 1Loss : 0.690578, Accuracy : 0.578125Pre-Clip param norm: 0.089817\n",
      "Training results of batch 50Loss : 0.691178, Accuracy : 0.484375Pre-Clip param norm: 0.059270\n",
      "Training results of batch 100Loss : 0.695871, Accuracy : 0.406250Pre-Clip param norm: 0.063831\n",
      "Training results of batch 150Loss : 0.702213, Accuracy : 0.500000Pre-Clip param norm: 0.098106\n",
      "Training results of batch 200Loss : 0.674775, Accuracy : 0.578125Pre-Clip param norm: 0.079176\n",
      "Training results of batch 250Loss : 0.689940, Accuracy : 0.500000Pre-Clip param norm: 0.111646\n",
      "Epoch num: 1Train: loss 0.6871, acc 0.5438Val: loss 0.6990, acc 0.5284,ROC-AUC 0.521,PR-AUC 0.556Best Thr(F1) 0.564, F1 -1.000, P 0.000, R 0.000, PosRate 0.535\n",
      "New Model saved: PR-AUC = 0.5563 (t*=0.564)\n",
      "Training results of batch 1Loss : 0.707619, Accuracy : 0.500000Pre-Clip param norm: 0.191558\n",
      "Training results of batch 50Loss : 0.694645, Accuracy : 0.500000Pre-Clip param norm: 0.232904\n",
      "Training results of batch 100Loss : 0.696365, Accuracy : 0.500000Pre-Clip param norm: 0.114182\n",
      "Training results of batch 150Loss : 0.674832, Accuracy : 0.515625Pre-Clip param norm: 0.152964\n",
      "Training results of batch 200Loss : 0.697610, Accuracy : 0.515625Pre-Clip param norm: 0.139356\n",
      "Training results of batch 250Loss : 0.651313, Accuracy : 0.671875Pre-Clip param norm: 0.196160\n",
      "Epoch num: 2Train: loss 0.6787, acc 0.5607Val: loss 0.6990, acc 0.5335,ROC-AUC 0.544,PR-AUC 0.575Best Thr(F1) 0.540, F1 0.596, P 0.556, R 0.641, PosRate 0.617\n",
      "New Model saved: PR-AUC = 0.5747 (t*=0.540)\n",
      "Training results of batch 1Loss : 0.703597, Accuracy : 0.546875Pre-Clip param norm: 0.162535\n",
      "Training results of batch 50Loss : 0.668740, Accuracy : 0.546875Pre-Clip param norm: 0.534328\n",
      "Training results of batch 100Loss : 0.692325, Accuracy : 0.484375Pre-Clip param norm: 0.136306\n",
      "Training results of batch 150Loss : 0.673018, Accuracy : 0.625000Pre-Clip param norm: 0.421463\n",
      "Training results of batch 200Loss : 0.659541, Accuracy : 0.625000Pre-Clip param norm: 0.314712\n",
      "Training results of batch 250Loss : 0.657357, Accuracy : 0.562500Pre-Clip param norm: 0.399881\n",
      "Epoch num: 3Train: loss 0.6630, acc 0.5905Val: loss 0.7184, acc 0.5394,ROC-AUC 0.539,PR-AUC 0.572Best Thr(F1) 0.525, F1 0.608, P 0.562, R 0.662, PosRate 0.631\n",
      "Training results of batch 1Loss : 0.667782, Accuracy : 0.593750Pre-Clip param norm: 0.605433\n",
      "Training results of batch 50Loss : 0.604261, Accuracy : 0.765625Pre-Clip param norm: 0.590409\n",
      "Training results of batch 100Loss : 0.657716, Accuracy : 0.593750Pre-Clip param norm: 0.348807\n",
      "Training results of batch 150Loss : 0.631199, Accuracy : 0.640625Pre-Clip param norm: 0.360391\n",
      "Training results of batch 200Loss : 0.623592, Accuracy : 0.671875Pre-Clip param norm: 0.896422\n",
      "Training results of batch 250Loss : 0.673286, Accuracy : 0.562500Pre-Clip param norm: 0.398076\n",
      "Epoch num: 4Train: loss 0.6424, acc 0.6233Val: loss 0.7270, acc 0.5523,ROC-AUC 0.551,PR-AUC 0.575Best Thr(F1) 0.540, F1 0.611, P 0.569, R 0.660, PosRate 0.622\n",
      "New Model saved: PR-AUC = 0.5749 (t*=0.540)\n",
      "Training results of batch 1Loss : 0.675832, Accuracy : 0.593750Pre-Clip param norm: 0.678912\n",
      "Training results of batch 50Loss : 0.638009, Accuracy : 0.609375Pre-Clip param norm: 0.824333\n",
      "Training results of batch 100Loss : 0.642816, Accuracy : 0.593750Pre-Clip param norm: 0.814783\n",
      "Training results of batch 150Loss : 0.613898, Accuracy : 0.609375Pre-Clip param norm: 1.023527\n",
      "Training results of batch 200Loss : 0.627162, Accuracy : 0.609375Pre-Clip param norm: 0.713308\n",
      "Training results of batch 250Loss : 0.513696, Accuracy : 0.750000Pre-Clip param norm: 0.727507\n",
      "Epoch num: 5Train: loss 0.6132, acc 0.6566Val: loss 0.7469, acc 0.5472,ROC-AUC 0.535,PR-AUC 0.560Best Thr(F1) 0.515, F1 0.608, P 0.564, R 0.659, PosRate 0.626\n",
      "Training results of batch 1Loss : 0.651726, Accuracy : 0.625000Pre-Clip param norm: 0.976994\n",
      "Training results of batch 50Loss : 0.586848, Accuracy : 0.718750Pre-Clip param norm: 1.239414\n",
      "Training results of batch 100Loss : 0.562097, Accuracy : 0.703125Pre-Clip param norm: 1.477463\n",
      "Training results of batch 150Loss : 0.570370, Accuracy : 0.718750Pre-Clip param norm: 0.841804\n",
      "Training results of batch 200Loss : 0.643531, Accuracy : 0.578125Pre-Clip param norm: 0.912526\n",
      "Training results of batch 250Loss : 0.605767, Accuracy : 0.640625Pre-Clip param norm: 1.442904\n",
      "Epoch num: 6Train: loss 0.5945, acc 0.6748Val: loss 0.7593, acc 0.5351,ROC-AUC 0.531,PR-AUC 0.562Best Thr(F1) 0.480, F1 0.602, P 0.555, R 0.657, PosRate 0.635\n",
      "Training results of batch 1Loss : 0.589389, Accuracy : 0.734375Pre-Clip param norm: 1.429930\n",
      "Training results of batch 50Loss : 0.622284, Accuracy : 0.640625Pre-Clip param norm: 1.048314\n",
      "Training results of batch 100Loss : 0.572518, Accuracy : 0.703125Pre-Clip param norm: 1.478580\n",
      "Training results of batch 150Loss : 0.520278, Accuracy : 0.671875Pre-Clip param norm: 1.250155\n",
      "Training results of batch 200Loss : 0.579249, Accuracy : 0.640625Pre-Clip param norm: 1.704482\n",
      "Training results of batch 250Loss : 0.577845, Accuracy : 0.625000Pre-Clip param norm: 0.801192\n",
      "Epoch num: 7Train: loss 0.5830, acc 0.6825Val: loss 0.7926, acc 0.5362,ROC-AUC 0.532,PR-AUC 0.561Best Thr(F1) 0.435, F1 0.607, P 0.560, R 0.662, PosRate 0.632\n",
      "Training results of batch 1Loss : 0.522758, Accuracy : 0.734375Pre-Clip param norm: 0.982890\n",
      "Training results of batch 50Loss : 0.603293, Accuracy : 0.687500Pre-Clip param norm: 2.153963\n",
      "Training results of batch 100Loss : 0.577289, Accuracy : 0.640625Pre-Clip param norm: 2.114277\n",
      "Training results of batch 150Loss : 0.449730, Accuracy : 0.812500Pre-Clip param norm: 1.718655\n",
      "Training results of batch 200Loss : 0.516207, Accuracy : 0.750000Pre-Clip param norm: 1.497269\n",
      "Training results of batch 250Loss : 0.603784, Accuracy : 0.640625Pre-Clip param norm: 1.152692\n",
      "Epoch num: 8Train: loss 0.5622, acc 0.6995Val: loss 0.7950, acc 0.5370,ROC-AUC 0.537,PR-AUC 0.569Best Thr(F1) 0.475, F1 0.602, P 0.555, R 0.658, PosRate 0.635\n",
      "Training results of batch 1Loss : 0.554610, Accuracy : 0.687500Pre-Clip param norm: 1.860589\n",
      "Training results of batch 50Loss : 0.534529, Accuracy : 0.718750Pre-Clip param norm: 1.787279\n",
      "Training results of batch 100Loss : 0.617358, Accuracy : 0.625000Pre-Clip param norm: 2.027378\n",
      "Training results of batch 150Loss : 0.527158, Accuracy : 0.781250Pre-Clip param norm: 4.694679\n",
      "Training results of batch 200Loss : 0.544231, Accuracy : 0.750000Pre-Clip param norm: 0.990835\n",
      "Training results of batch 250Loss : 0.706701, Accuracy : 0.531250Pre-Clip param norm: 2.725615\n",
      "Epoch num: 9Train: loss 0.5527, acc 0.7068Val: loss 0.8213, acc 0.5308,ROC-AUC 0.532,PR-AUC 0.564Best Thr(F1) 0.445, F1 0.600, P 0.555, R 0.653, PosRate 0.631\n",
      "Training results of batch 1Loss : 0.533292, Accuracy : 0.718750Pre-Clip param norm: 1.322799\n",
      "Training results of batch 50Loss : 0.546182, Accuracy : 0.796875Pre-Clip param norm: 2.233429\n",
      "Training results of batch 100Loss : 0.504942, Accuracy : 0.671875Pre-Clip param norm: 5.188960\n",
      "Training results of batch 150Loss : 0.563198, Accuracy : 0.718750Pre-Clip param norm: 1.953526\n",
      "Training results of batch 200Loss : 0.522650, Accuracy : 0.765625Pre-Clip param norm: 2.095784\n",
      "Training results of batch 250Loss : 0.589042, Accuracy : 0.609375Pre-Clip param norm: 2.374942\n",
      "Epoch num: 10Train: loss 0.5444, acc 0.7173Val: loss 0.8405, acc 0.5372,ROC-AUC 0.523,PR-AUC 0.550Best Thr(F1) 0.445, F1 0.600, P 0.553, R 0.656, PosRate 0.635\n",
      "Training results of batch 1Loss : 0.531464, Accuracy : 0.750000Pre-Clip param norm: 1.719464\n",
      "Training results of batch 50Loss : 0.555540, Accuracy : 0.687500Pre-Clip param norm: 2.078625\n",
      "Training results of batch 100Loss : 0.576803, Accuracy : 0.687500Pre-Clip param norm: 1.288101\n",
      "Training results of batch 150Loss : 0.619504, Accuracy : 0.609375Pre-Clip param norm: 1.794520\n",
      "Training results of batch 200Loss : 0.514887, Accuracy : 0.750000Pre-Clip param norm: 1.050636\n",
      "Training results of batch 250Loss : 0.477507, Accuracy : 0.796875Pre-Clip param norm: 1.152309\n",
      "Epoch num: 11Train: loss 0.5354, acc 0.7234Val: loss 0.8680, acc 0.5348,ROC-AUC 0.528,PR-AUC 0.556Best Thr(F1) 0.425, F1 0.605, P 0.558, R 0.661, PosRate 0.635\n",
      "Ended on epoch 11, due to patience.\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMClassifier\n",
    "from train.train_test_model import fit , positive_weight, select_threshold_constrained, threshold_free_metrics\n",
    "\n",
    "input_size = x_train.size(-1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(input_size=input_size,hidden_size=128,num_layers= 2)\n",
    "pos_weight = positive_weight(y_train,device)\n",
    "model,t_star,history = fit(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=40,\n",
    "        lr=1e-3,              \n",
    "        device=device,\n",
    "        save_path=\"models/best.pt\",\n",
    "        patience= 7,            # optional early stopping\n",
    "        pos_weight= None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325a9c4",
   "metadata": {},
   "source": [
    "Beginning of HyperTuning - Testing to see the model can learn and if changing things elsewhere (data/features/splitting/optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0050 | loss 0.0006 | acc 1.000\n",
      "0100 | loss 0.0001 | acc 1.000\n",
      "0150 | loss 0.0001 | acc 1.000\n",
      "0200 | loss 0.0000 | acc 1.000\n",
      "0250 | loss 0.0000 | acc 1.000\n",
      "0300 | loss 0.0000 | acc 1.000\n",
      "0350 | loss 0.0000 | acc 1.000\n",
      "0400 | loss 0.0000 | acc 1.000\n",
      "0450 | loss 0.0000 | acc 1.000\n",
      "0500 | loss 0.0000 | acc 1.000\n"
     ]
    }
   ],
   "source": [
    "def tiny_overfit(model, train_ds, steps=500, k=64, lr=3e-3):\n",
    "    from torch.optim import AdamW\n",
    "    from torch.nn.utils import clip_grad_norm_\n",
    "    import random, torch, torch.nn as nn\n",
    "    idx = torch.randperm(len(train_ds))[:k]\n",
    "    X_small = []; y_small = []\n",
    "    for i in idx:\n",
    "        X, y = train_ds[i]\n",
    "        X_small.append(X.unsqueeze(0)); y_small.append(y)\n",
    "    X_small = torch.cat(X_small, dim=0)  # [k, T, F]\n",
    "    y_small = torch.tensor(y_small).float()  # [k]\n",
    "\n",
    "    model.train()\n",
    "    opt = AdamW(model.parameters(), lr=lr)\n",
    "    crit = torch.nn.BCEWithLogitsLoss()\n",
    "    for t in range(steps):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(X_small.to(device))\n",
    "        loss = crit(logits.squeeze(-1), y_small.to(device))\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        if (t+1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                p = (logits.squeeze(-1).sigmoid()>0.5).float()\n",
    "                acc = (p.cpu()==y_small).float().mean().item()\n",
    "            print(f'{t+1:04d} | loss {loss.item():.4f} | acc {acc:.3f}')\n",
    "\n",
    "tiny_overfit(\n",
    "    model=model,\n",
    "    train_ds=SequenceDataset(x_train,y_train),\n",
    "    steps=500,   # updates, not epochs\n",
    "    k=64,        # number of samples to memorize\n",
    "    lr=3e-3      # slightly higher LR for speed\n",
    ")\n",
    "\n",
    "#FROM THIS CELLS OUTPUT OF AN ACCURACY OF 1+ CONSISTENLY AND LOSS OF 0  SO MODEL CAN DEFINETLY LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b55485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg VAL: {'roc': 0.5608382604492042, 'pr': 0.5825557162074742, 't': 0.4599999999999999, 'f1': 0.6708587148752211, 'prec': 0.5511785598966742, 'rec': 0.8569277108433735, 'pos_rate': 0.8325268817204301}\n",
      "LogReg TEST: {'roc': 0.6011337047410288, 'pr': 0.6291387219592178, 'f1': 0.6142034548944337, 'prec': 0.5799728137743543, 'rec': 0.6527281998980112, 'pos_rate': 0.5932795698924731, 't': 0.4599999999999999}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Test a regression model on data\n",
    "\n",
    "def make_tabular_last(norm_df, window_size=48):\n",
    "    feats = norm_df.drop(columns=['label'])\n",
    "    y = norm_df['label'].to_numpy().astype(int)\n",
    "    Xrows, yrows = [], []\n",
    "    for i in range(len(norm_df) - window_size + 1):\n",
    "        j = i + window_size - 1\n",
    "        Xrows.append(feats.iloc[j].to_numpy())\n",
    "        yrows.append(y[j])\n",
    "    X = np.vstack(Xrows); y = np.array(yrows)\n",
    "    return X, y\n",
    "\n",
    "def chrono_split(X, y, train_frac=0.7, val_frac=0.15):\n",
    "    n = len(y)\n",
    "    i_tr = int(n * train_frac)\n",
    "    i_v  = int(n * (train_frac + val_frac))\n",
    "    return (X[:i_tr], y[:i_tr]), (X[i_tr:i_v], y[i_tr:i_v]), (X[i_v:], y[i_v:])\n",
    "\n",
    "def eval_with_constraints(probs, y, min_pos_rate=0.05, max_pos_rate=0.95, min_precision=None):\n",
    "    roc, pr = threshold_free_metrics(probs, y)\n",
    "    tinfo = select_threshold_constrained(probs, y, min_pos_rate, max_pos_rate, min_precision)\n",
    "    if tinfo[\"f1\"] < 0:\n",
    "        return {\"roc\": roc, \"pr\": pr, \"t\": None, \"f1\": None, \"prec\": None, \"rec\": None, \"pos_rate\": None}\n",
    "    yhat = (probs >= tinfo[\"t\"])\n",
    "    return {\n",
    "        \"roc\": roc, \"pr\": pr, \"t\": tinfo[\"t\"], \"f1\": tinfo[\"f1\"],\n",
    "        \"prec\": precision_score(y, yhat, zero_division=0),\n",
    "        \"rec\":  recall_score(y, yhat, zero_division=0),\n",
    "        \"pos_rate\": float(yhat.mean())\n",
    "    }\n",
    "\n",
    "def logistic_baseline(norm_df, window_size=48, C=1.0, max_iter=2000):\n",
    "    X, y = make_tabular_last(norm_df, window_size)\n",
    "    (Xtr, ytr), (Xv, yv), (Xte, yte) = chrono_split(X, y, 0.7, 0.15)\n",
    "    cls_wt = \"balanced\" if (ytr.mean() < 0.35 or ytr.mean() > 0.65) else None\n",
    "    lr = LogisticRegression(C=C, max_iter=max_iter, solver=\"lbfgs\", class_weight=cls_wt)\n",
    "    lr.fit(Xtr, ytr)\n",
    "\n",
    "    pv = lr.predict_proba(Xv)[:,1]\n",
    "    min_prec = max(0.55, float(yv.mean()))\n",
    "    val_metrics = eval_with_constraints(pv, yv, 0.05, 0.95, min_prec)\n",
    "\n",
    "    pt = lr.predict_proba(Xte)[:,1]\n",
    "    if val_metrics[\"t\"] is not None:\n",
    "        yhat_t = (pt >= val_metrics[\"t\"])\n",
    "        test_metrics = {\n",
    "            \"roc\": roc_auc_score(yte, pt),\n",
    "            \"pr\":  average_precision_score(yte, pt),\n",
    "            \"f1\":  f1_score(yte, yhat_t, zero_division=0),\n",
    "            \"prec\": precision_score(yte, yhat_t, zero_division=0),\n",
    "            \"rec\":  recall_score(yte, yhat_t, zero_division=0),\n",
    "            \"pos_rate\": float(yhat_t.mean()),\n",
    "            \"t\": val_metrics[\"t\"],\n",
    "        }\n",
    "    else:\n",
    "        test_metrics = {\"roc\": roc_auc_score(yte, pt), \"pr\": average_precision_score(yte, pt),\n",
    "                        \"f1\": None, \"prec\": None, \"rec\": None, \"pos_rate\": None, \"t\": None}\n",
    "    return val_metrics, test_metrics\n",
    "\n",
    "val_m, test_m = logistic_baseline(norm_df, window_size=48)\n",
    "print(\"LogReg VAL:\", val_m)\n",
    "print(\"LogReg TEST:\", test_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de113e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k  eps_q   VAL_PR   VAL_ROC   TEST_PR  TEST_ROC\n",
      "6  0.6    0.560    0.548     0.572    0.545\n",
      "6  0.7    0.558    0.571     0.582    0.565\n",
      "8  0.6    0.568    0.554     0.585    0.551\n",
      "8  0.7    0.575    0.567     0.594    0.565\n",
      "12 0.6    0.588    0.574     0.603    0.568\n",
      "12 0.7    0.578    0.580     0.648    0.594\n",
      "\n",
      "BEST (by VAL_PR): k=12, eps_q=0.60  VAL_PR=0.588  VAL_ROC=0.574\n"
     ]
    }
   ],
   "source": [
    "feat_df = feature_creation(df) \n",
    "\n",
    "#Try a few labels: k in {6,8,12}, epsilon= 60% or 70% \n",
    "tests = []\n",
    "for k in (6, 8, 12):\n",
    "    for q in (0.60, 0.70):\n",
    "        labeled = make_label_k_epsilon(feat_df, k=k, eps_quantile=q, train_frac=0.7, use_log_returns=True)\n",
    "        norm = z_score_norm(labeled, train_frac=0.7)\n",
    "\n",
    "        # Find baseline results for comapring\n",
    "        val_m, test_m = logistic_baseline(norm, window_size=48)  \n",
    "        tests.append((k, q, val_m[\"pr\"], val_m[\"roc\"], test_m[\"pr\"], test_m[\"roc\"]))\n",
    "\n",
    "\n",
    "print(\"k  eps_q   VAL_PR   VAL_ROC   TEST_PR  TEST_ROC\")\n",
    "for k, q, vpr, vroc, tpr, troc in tests:\n",
    "    print(f\"{k:<2} {q:<5}  {vpr:.3f}    {vroc:.3f}     {tpr:.3f}    {troc:.3f}\")\n",
    "\n",
    "\n",
    "best = max(tests, key=lambda x: x[2])\n",
    "print(\"\\nBEST (by VAL_PR): k=%d, eps_q=%.2f  VAL_PR=%.3f  VAL_ROC=%.3f\" % (best[0], best[1], best[2], best[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bc51a",
   "metadata": {},
   "source": [
    "6. Testing the model on test data set and comparing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fca76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST REPORT: {'roc': 0.6053970258576201, 'pr': 0.6294132388074424, 'f1': 0.40837336993822926, 'prec': 0.625, 'rec': 0.30326197757390416, 'pos_rate': 0.25591397849462366, 't': 0.5399999999999999, 'prevalence': 0.5274193548387097, 'n': 3720}\n",
      "{'TP': 595, 'FP': 357, 'TN': 1401, 'FN': 1367}\n"
     ]
    }
   ],
   "source": [
    "from train.train_test_model import evaluate_test \n",
    "\n",
    "model.load_state_dict(torch.load(\"models/best.pt\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "test_report, test_probs, test_y = evaluate_test(model, test_loader, device, t_star)\n",
    "print(\"TEST REPORT:\", test_report)\n",
    "\n",
    "# (optional) confusion matrix counts\n",
    "yhat = (test_probs >= test_report[\"t\"])\n",
    "tn = int(((yhat == 0) & (test_y == 0)).sum())\n",
    "tp = int(((yhat == 1) & (test_y == 1)).sum())\n",
    "fn = int(((yhat == 0) & (test_y == 1)).sum())\n",
    "fp = int(((yhat == 1) & (test_y == 0)).sum())\n",
    "print({\"True Pos\": tp, \"False Pos\": fp, \"True Neg\": tn, \"False Neg\": fn})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
